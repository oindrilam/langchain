{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "947d5e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (0.0.181)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from langchain) (0.5.7)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from langchain) (1.21.5)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from langchain) (2.27.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from langchain) (1.4.32)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from langchain) (1.10.8)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from langchain) (4.0.1)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from langchain) (2.8.4)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (21.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.6.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (5.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from async-timeout<5.0.0,>=4.0.0->langchain) (4.6.2)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (3.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2021.10.8)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (1.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (0.4.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e62f37e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (0.27.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from openai) (4.64.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from openai) (2.27.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2021.10.8)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from aiohttp->openai) (4.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.6.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from aiohttp->openai) (5.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from aiohttp->openai) (21.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from async-timeout<5.0,>=4.0.0a3->aiohttp->openai) (4.6.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from tqdm->openai) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a37090d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d850e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Learner Free Academy\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.9)  # model_name=\"text-davinci-003\"\n",
    "text = \"What would be a good company name for a company that teaches students for free?\"\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fda4e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nTress Trends Corporate Accessories.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(temperature=0.9)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")\n",
    "#We can now create a very simple chain that will take user input, format the prompt with it, and then send it to the LLM:\n",
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "#Now we can run that chain only specifying the product!\n",
    "chain.run(\"corporate hair clips\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2618032d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-search-results in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (2.4.2)\n",
      "Requirement already satisfied: requests in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from google-search-results) (2.27.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from requests->google-search-results) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from requests->google-search-results) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from requests->google-search-results) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from requests->google-search-results) (2.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a306f83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-serp-api in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: requests in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from google-serp-api) (2.27.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from requests->google-serp-api) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from requests->google-serp-api) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from requests->google-serp-api) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from requests->google-serp-api) (3.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google-serp-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5110d897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58567cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find the temperature first, then use the calculator to raise it to the .023 power.\n",
      "Action: Search\n",
      "Action Input: \"High temperature in SF yesterday\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mHigh: 69.8ºf @3:45 PM Low: 55.04ºf @5:56 AM Approx.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to use the calculator to raise 69.8 to the .023 power\n",
      "Action: Calculator\n",
      "Action Input: 69.8^.023\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 1.1025763550530143\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 1.1025763550530143\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.1025763550530143'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# First, let's load the language model we're going to use to control the agent.\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Now let's test it out!\n",
    "agent.run(\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34c560fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Hi there! It's nice to meet you. How can I help you today?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
      "Human: I'm doing well! Just having a conversation with an AI.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " That's great! It's always nice to have a conversation with someone new. What would you like to talk about?\n"
     ]
    }
   ],
   "source": [
    "from langchain import OpenAI, ConversationChain\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "output = conversation.predict(input=\"Hi there!\")\n",
    "print(output)\n",
    "output = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7123e21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a good name for a company that makes food?\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "print(prompt.format(product=\"food\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c1515d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51e8283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae791ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from wikipedia) (4.11.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from wikipedia) (2.27.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\oindrilamondal\\anaconda3\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d94e8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out the year the film was released and then use the calculator to calculate the power.\n",
      "Action: Wikipedia\n",
      "Action Input: \"Titanic (1997 film)\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Titanic (1997 film)\n",
      "Summary: Titanic is a 1997 American epic romance and disaster film directed, written, produced, and co-edited by James Cameron. Incorporating both historical and fictionalized aspects, it is based on accounts of the sinking of RMS Titanic and stars Kate Winslet and Leonardo DiCaprio as members of different social classes who fall in love aboard the ship during its ill-fated maiden voyage. The film also features Billy Zane, Kathy Bates, Frances Fisher, Gloria Stuart, Bernard Hill, Jonathan Hyde, Victor Garber, and Bill Paxton.\n",
      "Cameron's inspiration for the film came from his fascination with shipwrecks; he felt a love story interspersed with the human loss would be essential to convey the emotional impact of the disaster. Production began on September 1, 1995, when Cameron shot footage of the actual Titanic wreck. The modern scenes on the research vessel were shot on board the Akademik Mstislav Keldysh, which Cameron had used as a base when filming the wreck. Scale models, computer-generated imagery, and a reconstruction of the Titanic built at Baja Studios were used to re-create the sinking. The film was co-financed by Paramount Pictures and 20th Century Fox; the former handled distribution in North America while the latter released the film internationally. It was the most expensive film ever made at the time, with a production budget of $200 million. Filming took place from July 1996 to March 1997.\n",
      "Upon its release on December 19, 1997, Titanic achieved significant critical and commercial success, and then received numerous accolades. It was praised for its visual effects, performances (particularly DiCaprio, Winslet, and Stuart), production values, Cameron's direction, musical score, cinematography, story, and emotional depth. Nominated for 14 Academy Awards, it tied All About Eve (1950) for the most Oscar nominations, and won 11, including the awards for Best Picture and Best Director, tying Ben-Hur (1959) for the most Oscars won by a single film.  With an initial worldwide gross of over $1.84 billion, Titanic was the first film to reach the billion-dollar mark. It remained the highest-grossing film of all time until Cameron's next film, Avatar, surpassed it in 2010. A number of re-releases since 1997 have pushed the film's worldwide total to $2.257 billion, making it the second film to gross more than $2 billion worldwide (after Avatar). It was also selected for preservation in the United States National Film Registry in 2017 for being \"culturally, historically or aesthetically significant\".\n",
      "\n",
      "Page: Titanic 666\n",
      "Summary: Titanic 666 (also known as Titanic 3) is a 2022 American supernatural horror film directed by Nick Lyon and produced by The Asylum. It is a sequel to Titanic II (2010) and stars Jamie Bamber and Keesha Sharp. It was released on Tubi on the United States on April 15, 2022, on the 110th anniversary of the sinking of the Titanic. The film received generally unfavorable reviews, and users on social media noted the film's similarity to Titanic (1997) and Studio 666 (2022).\n",
      "\n",
      "\n",
      "\n",
      "Page: Titanic II (film)\n",
      "Summary: Titanic II (also styled Titanic 2) is a 2010 American drama disaster film written, directed by and starring Shane Van Dyke and distributed by The Asylum. Despite the title, it is not a sequel to the 1997 critically acclaimed film, but is a mockbuster of it. It was released direct-to-TV in Australia on 7 August 2010. It premiered on Syfy, on Sky in the UK and Ireland on 9 August. It was released on 25 August in the United States to critically negative response, though the film's ensemble cast performances, particularly that of Bruce Davison, received praise.\n",
      "The film is set on a fictional replica Titanic that sets off exactly 100 years after the original ship's maiden voyage to perform the reverse route, but global warming and the forces of nature cause history to repeat itself on the same night, only on a more disastrous and deadly scale. A supernatural horror-themed sequel, Titanic\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the year the film was released.\n",
      "Action: Calculator\n",
      "Action Input: 1997^0.43\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 26.25201041567534\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: The film Titanic with Leonardo Di Caprio was released in 1997 and 1997 raised to the 0.43 power is 26.25201041567534.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The film Titanic with Leonardo Di Caprio was released in 1997 and 1997 raised to the 0.43 power is 26.25201041567534.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(temperature=0)\n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n",
    "agent.run(\"In what year was the film Titanic with Leonardo Di Caprio released? What is this year raised to the 0.43 power?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e139e149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "226f793f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'aime programmer.\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat([HumanMessage(content=\"Translate this sentence from English to French. I love programming.\")])\n",
    "# -> AIMessage(content=\"J'aime programmer.\", additional_kwargs={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4242686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "    HumanMessage(content=\"I love programming.\")\n",
    "]\n",
    "chat(messages)\n",
    "# -> AIMessage(content=\"J'aime programmer.\", additional_kwargs={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9a1ba5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-vWg7XU7LTUa23IVU2puSx6nq on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-vWg7XU7LTUa23IVU2puSx6nq on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-vWg7XU7LTUa23IVU2puSx6nq on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-vWg7XU7LTUa23IVU2puSx6nq on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[ChatGeneration(text=\"J'adore la programmation.\", generation_info=None, message=AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, example=False))], [ChatGeneration(text=\"J'adore l'intelligence artificielle.\", generation_info=None, message=AIMessage(content=\"J'adore l'intelligence artificielle.\", additional_kwargs={}, example=False))]], llm_output={'token_usage': {'prompt_tokens': 57, 'completion_tokens': 20, 'total_tokens': 77}, 'model_name': 'gpt-3.5-turbo'})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_messages = [\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "        HumanMessage(content=\"I love programming.\")\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "        HumanMessage(content=\"I love artificial intelligence.\")\n",
    "    ],\n",
    "]\n",
    "result = chat.generate(batch_messages)\n",
    "result\n",
    "# -> LLMResult(generations=[[ChatGeneration(text=\"J'aime programmer.\", generation_info=None, message=AIMessage(content=\"J'aime programmer.\", additional_kwargs={}))], [ChatGeneration(text=\"J'aime l'intelligence artificielle.\", generation_info=None, message=AIMessage(content=\"J'aime l'intelligence artificielle.\", additional_kwargs={}))]], llm_output={'token_usage': {'prompt_tokens': 57, 'completion_tokens': 20, 'total_tokens': 77}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31999486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt_tokens': 57, 'completion_tokens': 20, 'total_tokens': 77}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.llm_output['token_usage']\n",
    "# -> {'prompt_tokens': 57, 'completion_tokens': 20, 'total_tokens': 77}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48609d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-vWg7XU7LTUa23IVU2puSx6nq on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-vWg7XU7LTUa23IVU2puSx6nq on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-vWg7XU7LTUa23IVU2puSx6nq on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-vWg7XU7LTUa23IVU2puSx6nq on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# get a chat completion from the formatted messages\n",
    "chat(chat_prompt.format_prompt(input_language=\"English\", output_language=\"French\", text=\"I love programming.\").to_messages())\n",
    "# -> AIMessage(content=\"J'aime programmer.\", additional_kwargs={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05edf481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-vWg7XU7LTUa23IVU2puSx6nq on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-vWg7XU7LTUa23IVU2puSx6nq on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-vWg7XU7LTUa23IVU2puSx6nq on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-vWg7XU7LTUa23IVU2puSx6nq on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 16.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-vWg7XU7LTUa23IVU2puSx6nq on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"J'adore la programmation.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "chain.run(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")\n",
    "# -> \"J'aime programmer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51216acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-vWg7XU7LTUa23IVU2puSx6nq on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-vWg7XU7LTUa23IVU2puSx6nq on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "ename": "OutputParserException",
     "evalue": "Could not parse LLM output: Thought: I need to use a search engine to find Olivia Wilde's boyfriend and a calculator to raise his age to the 0.23 power.\nAction:\n```\n{\n  \"action\": \"Search\",\n  \"action_input\": \"Olivia Wilde boyfriend\"\n}\n```\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\agents\\chat\\output_parser.py:21\u001b[0m, in \u001b[0;36mChatOutputParser.parse\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 21\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mparse_json_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m AgentAction(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m\"\u001b[39m], response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction_input\u001b[39m\u001b[38;5;124m\"\u001b[39m], text)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\output_parsers\\json.py:17\u001b[0m, in \u001b[0;36mparse_json_markdown\u001b[1;34m(json_string)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Parse the JSON string into a Python dictionary\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m parsed \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parsed\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m agent \u001b[38;5;241m=\u001b[39m initialize_agent(tools, chat, agent\u001b[38;5;241m=\u001b[39mAgentType\u001b[38;5;241m.\u001b[39mCHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Now let's test it out!\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWho is Olivia Wilde\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms boyfriend? What is his current age raised to the 0.23 power?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py:236\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, *args, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks)[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys[\u001b[38;5;241m0\u001b[39m]]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py:140\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    139\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    141\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py:134\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[0;32m    128\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m    129\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    130\u001b[0m     inputs,\n\u001b[0;32m    131\u001b[0m )\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 134\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    136\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    137\u001b[0m     )\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    139\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\agents\\agent.py:951\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    949\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[0;32m    950\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[1;32m--> 951\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[0;32m    960\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[0;32m    961\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\agents\\agent.py:773\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m    771\u001b[0m     raise_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_error:\n\u001b[1;32m--> 773\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    774\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[0;32m    775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\agents\\agent.py:762\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Take a single step in the thought-action-observation loop.\u001b[39;00m\n\u001b[0;32m    757\u001b[0m \n\u001b[0;32m    758\u001b[0m \u001b[38;5;124;03mOverride this to take control of how the agent makes and acts on choices.\u001b[39;00m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    760\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    761\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[1;32m--> 762\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mplan(\n\u001b[0;32m    763\u001b[0m         intermediate_steps,\n\u001b[0;32m    764\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    765\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[0;32m    766\u001b[0m     )\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    768\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\agents\\agent.py:444\u001b[0m, in \u001b[0;36mAgent.plan\u001b[1;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    442\u001b[0m full_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_full_inputs(intermediate_steps, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    443\u001b[0m full_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mpredict(callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfull_inputs)\n\u001b[1;32m--> 444\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\agents\\chat\\output_parser.py:25\u001b[0m, in \u001b[0;36mChatOutputParser.parse\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m AgentAction(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m\"\u001b[39m], response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction_input\u001b[39m\u001b[38;5;124m\"\u001b[39m], text)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not parse LLM output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mOutputParserException\u001b[0m: Could not parse LLM output: Thought: I need to use a search engine to find Olivia Wilde's boyfriend and a calculator to raise his age to the 0.23 power.\nAction:\n```\n{\n  \"action\": \"Search\",\n  \"action_input\": \"Olivia Wilde boyfriend\"\n}\n```\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# First, let's load the language model we're going to use to control the agent.\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
    "llm = OpenAI(temperature=0)\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Now let's test it out!\n",
    "agent.run(\"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "decd1e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello! I am an AI language model created by OpenAI. I don't have a specific name, but you can call me OpenAI if you'd like.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate, \n",
    "    MessagesPlaceholder, \n",
    "    SystemMessagePromptTemplate, \n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\n",
    "\n",
    "conversation.predict(input=\"Hello there! What is your name?\")\n",
    "# -> 'Hello! How can I assist you today?'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f843ff2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure, I'd be happy to help! Hyderabad is a beautiful city with many popular tourist attractions. Here are some of the top places to visit in Hyderabad:\\n\\n1. Charminar - This iconic monument is a must-visit for anyone traveling to Hyderabad. It was built in 1591 and is a symbol of the city's rich history and culture.\\n\\n2. Golconda Fort - This historic fort is located on a hilltop and offers stunning views of the city. It was built in the 16th century and is known for its impressive architecture and acoustics.\\n\\n3. Hussain Sagar Lake - This man-made lake is a popular spot for boating and offers beautiful views of the city. It also has a large statue of Buddha in the center.\\n\\n4. Ramoji Film City - This is the world's largest film studio complex and is a popular tourist destination. It offers a variety of attractions, including movie sets, amusement parks, and live shows.\\n\\n5. Salar Jung Museum - This museum is home to one of the largest collections of art and artifacts in the world. It has over 1 million objects, including paintings, sculptures, and textiles.\\n\\nI hope this helps! Let me know if you have any other questions.\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Okay, I will call you AIBuddy. So, AIBuddy can you tell me top tourist places in Hyderabad, India?\")\n",
    "# -> \"That sounds like fun! I'm happy to chat with you. Is there anything specific you'd like to talk about?\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf00cb4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Switching to a data science role can be a great career move, but it does require some preparation and effort. Here are some steps you can take to make the transition:\\n\\n1. Learn the necessary skills: Data science requires a combination of technical and analytical skills. You should have a strong foundation in statistics, programming languages like Python or R, and data visualization tools like Tableau or Power BI. You can take online courses, attend bootcamps, or pursue a degree in data science to gain these skills.\\n\\n2. Build a portfolio: Once you have the necessary skills, you should start building a portfolio of data science projects. This will demonstrate your abilities to potential employers and give you hands-on experience working with real-world data.\\n\\n3. Network with other data scientists: Attend meetups, conferences, and other events to connect with other data scientists. This can help you learn about job opportunities and get advice on how to break into the field.\\n\\n4. Apply for data science jobs: Once you have the skills and portfolio, start applying for data science jobs. Look for entry-level positions or internships to get your foot in the door. Be prepared to showcase your portfolio and explain how your skills and experience make you a good fit for the role.\\n\\n5. Keep learning: Data science is a rapidly evolving field, so it's important to stay up-to-date with the latest tools and techniques. Continue taking courses, attending events, and working on personal projects to keep your skills sharp.\\n\\nI hope this helps! Let me know if you have any other questions.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Tell me how to switch to a data science role in career.\")\n",
    "# -> \"Sure! I am an AI language model created by OpenAI. I was trained on a large dataset of text from the internet, which allows me to understand and generate human-like language. I can answer questions, provide information, and even have conversations like this one. Is there anything else you'd like to know about me?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a87df19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are many companies that are investing heavily in data science and analytics to gain insights and make data-driven decisions. Here are some of the top data science companies:\\n\\n1. Amazon - Amazon is one of the largest e-commerce companies in the world and uses data science extensively to personalize recommendations, optimize pricing, and improve customer experience.\\n\\n2. Google - Google is a technology giant that uses data science to power its search engine, advertising platform, and other products. It also invests heavily in machine learning and artificial intelligence research.\\n\\n3. Microsoft - Microsoft is a software company that uses data science to improve its products and services, including its cloud computing platform Azure. It also has a dedicated data science team that works on research and development projects.\\n\\n4. Facebook - Facebook is a social media company that uses data science to personalize content, target ads, and improve user engagement. It also invests in machine learning and artificial intelligence research.\\n\\n5. IBM - IBM is a technology and consulting company that uses data science to help clients solve complex business problems. It also has a dedicated data science team that works on research and development projects.\\n\\n6. Airbnb - Airbnb is a hospitality company that uses data science to optimize pricing, improve search results, and personalize recommendations for its users.\\n\\n7. Uber - Uber is a transportation company that uses data science to optimize routes, predict demand, and improve driver and rider experiences.\\n\\nThese are just a few examples of companies that are investing in data science. There are many other companies across industries that are using data science to gain insights and make data-driven decisions.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Tell me the top data science companies.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6683ba51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "llm.predict_messages([HumanMessage(content=\"say hi!\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "073d2a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello there!', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "#llm = OpenAI()\n",
    "chat_model = ChatOpenAI()\n",
    "chat_model.predict_messages([HumanMessage(content=\"say hi!\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3ab73aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm=OpenAI(model_name=\"text-ada-001\",n=2,best_of=2)\n",
    "llm(\"Tell me a joke.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7830b275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=\"\\n\\nOf the time we spent together,\\n\\nAnd the love that we found,\\n\\nI can't imagine without you\\n\\nWe would have the time of our lives\\n\\nAnd then what came together\\n\\nMy love for you was strong\\n\\nAnd we kept going no matter what\\n\\nWe were meant to be together\\n\\nAnd we made it work for both of us\\n\\nYou are the one I want to love\\n\\nAnd I can't imagine without you\\n\\nI want to love you too\\n\\nWith all my heart\\n\\nWe spent hours together\\n\\nAnd I felt like I was home\\n\\nAnd I was so happy\\n\\nThat we spent together\\n\\nA time that I would never give away\\n\\nTo anyone else\\n\\nI love you\", generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text=\"\\n\\nHow can a rose be love\\n\\nWhen it's been ripped apart\\n\\nAnd all the room's filled with dirt\\n\\nWith its thorns,\\n\\nHow can a rose be love\\n\\nWhen its petals are covered in dust\\n\\nAnd its seeds are hidden in rain\\n\\nHow can a rose be love\\n\\nWhen its petals are covered in dirt\\n\\nAnd its seeds are hidden in rain\\n\\nAnd how can it be when its petals are covered in dust\\n\\nAnd how can it be when its seeds are hidden in rain\\n\\nWhen its petals are covered in dust\\n\\nAnd its seeds are hidden in rain\\n\\nHow can a rose be love\\n\\nWhen its petals are covered in dirt\\n\\nAnd its seeds are hidden in rain\\n\\nWhen its seeds are hidden in rain\\n\\nHow can a rose be love\\n\\nWhen its petals are covered in dirt\\n\\nAnd its seeds are hidden in rain\\n\\nWhen its seeds are hidden in rain\\n\\nHow can a rose be love\", generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=\"\\n\\nWhen I was younger\\nI thought that love\\nI was something like a fairytale\\nI would find my prince\\nAnd we would be together\\nForever\\nI was naïve\\nAnd I was wrong\\nLove is not a fairytale\\nIt's something else entirely\\nSomething that should be cherished\\nAnd loved\\nAnd never taken for granted\\nLove is something that you have to work for\\nIt doesn't come easy\\nYou have to sacrifice\\nYour time, your effort\\nAnd sometimes you have to give up \\nYou have to do what's best for yourself\\nAnd sometimes that means giving love up\", generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text=\"\\n\\nHow do you describe the\\n\\nThe way you see the world\\n\\nThe way you live your life\\n\\nThe way you want to live it?\\n\\nIt's easy to understand\\n\\nWhen you can communicate\\n\\nYour vision is clear\\n\\nAnd people can see\\n\\nYour vision is clear\\n\\nAnd they can see\\n\\nYour vision is clear\\n\\nAnd they can see\\n\\nYour vision is clear\\n\\nAnd they can see\\n\\nThe way you see the world\\n\\nThe way you live your life\\n\\nThe way you want to live it?\\n\\nIt's easy to understand\\n\\nWhen you can communicate\\n\\nYour vision is clear\\n\\nAnd people can see\\n\\nYour vision is clear\\n\\nAnd they can see\\n\\nYour vision is clear\\n\\nAnd they can see\\n\\nThe way you see the world\\n\\nThe way you live your life\\n\\nThe way you want to live it?\", generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWe are all broken\\n\\nEach piece of us\\n\\nBleeds out in the rain\\n\\nBut we try to mend\\n\\nAnd hope for a better tomorrow.\\n\\nOur dreams and hopes\\n\\nStill linger in the air\\n\\nBut we don’t know what to do\\n\\nWith the pain and scars\\n\\nWe try to hide\\n\\nBut we can’t keep our minds from\\n\\nReminding us of our past.', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nI am not sure\\nI am not sure\\n\\nOf what I am feeling\\n\\nI am not sure\\n\\nOf what I am feeling\\n\\nI am not sure\\n\\nOf what I am feeling\\n\\nI am not sure\\n\\nOf what I am feeling\\n\\nI am not sure\\n\\nOf what I am feeling\\n\\nI am not sure\\n\\nOf what I am feeling\\n\\nI am not sure\\n\\nOf what I am feeling\\n\\nI am not sure\\n\\nOf what I am feeling\\n\\nI am not sure\\n\\nOf what I am feeling\\n\\nI am not sure\\n\\nOf what I am feeling\\n\\nI am not sure\\n\\nOf what I am feeling\\n\\nI am not sure\\n\\nOf what I am feeling', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=\"\\n\\nWhen I was younger\\nI thought that love\\nI was something like a fairytale\\nI would find my prince\\nAnd we would be together\\nForever\\nI was naïve\\nAnd I was wrong\\nLove is not a fairytale\\nIt's something else entirely\\nSomething that should be cherished\\nAnd loved\\nAnd never taken for granted\\nLove is something that you have to work for\\nIt doesn't come easy\\nYou have to sacrifice\\nYour time, your effort\\nAnd sometimes you have to give up \\nYou have to do what's best for yourself\\nAnd sometimes that means giving love up\", generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text=\"\\n\\nHow can I make you my friend\\nWhen I can't be there to help you\\nYou always are there to help me\\nIn the dark of the night\\nWe can always find each other\\nIn a place where there's light\\nSo how can I make you my friend\\nWhen I can't be there to help you\\nYou always are there to help me\\nIn the dark of the night\\nWe can always find each other\\nIn a place where there's light\\nSo how can I make you my friend\\n\\nYou always are there to help me\\nIn the dark of the night\\nWe can always find each other\\nIn a place where there's light\", generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=\"\\n\\nI go about my life\\n\\nTrying to fit in\\n\\nBut I feel so small\\n\\nI can't help but feel scared\\n\\nBut I also go about my life\\n\\nTrying to find someone to rely on\\n\\nBut I don't know how to show myself\\n\\nI go about my life\\n\\nTrying to find a way to be okay\\n\\nBut I'm so scared of being okay\\n\\nAnd I go about my life\\n\\nTrying to find a purpose\\n\\nTo what? To being okay?\\n\\nI go about my life\\n\\nTrying to find a way to be okay\\n\\nBut I'm so scared of being okay\\n\\nAnd I go about my life\\n\\nTrying to find a way to be okay\\n\\nBut I'm so scared of being okay\\n\\nAnd I go about my life\\n\\nTrying to find a way to be okay\\n\\nBut I'm so scared of being okay\", generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text=\"\\n\\nIn the eyes of the moon\\n\\nI have seen a face\\n\\nThat I will never forget.\\n\\nIts beauty is so divine\\n\\nI can't imagine life without it.\\n\\nIn the night,\\n\\nIt glows\\n\\nLike a star\\n\\nAnd I can't help but love it.\", generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=\"\\n\\nA rose by the side of the road\\n\\nIs all I need to find my way\\n\\nTo the place I've been searching for\\n\\nAnd my heart is singing with joy\\n\\nWhen I look at this rose\\n\\nIt reminds me of the love I've found\\n\\nAnd I know that wherever I go\\n\\nI'll always find my rose by the side of the road.\", generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text=\"\\n\\nThe world is a beautiful place,\\nThe colors are so bright and true,\\nAnd I feel so free and free,\\nWhen I'm away from here.\\n\\nThe sky is so blue,\\nAnd the sun is so warm,\\nAnd I feel so free and free,\\nWhen I'm away from here.\", generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nI lay in my bed\\n\\nAnd I dreamed a dream\\n\\nOf a WHITE TOWER\\n\\nThat was,\\n\\nAnd I knew that I could be\\n\\nThestorytellerofmy dreams\\n\\nAnd I would never give up\\n\\nThe flag that was,\\n\\nAnd we would sit down\\n\\nTo eat lunch\\n\\nAnd the sun would be\\n\\nDull and I would be,\\n\\nAnd we would meet again\\n\\nIn a new building\\n\\nWith a new name\\n\\nAnd we would be\\n\\nmony and we would\\n\\nBeside each other\\n\\nIn a moment\\n\\nOf beauty\\n\\nThat we would never forget\\n\\nWe would share our dream\\n\\nOf a WHITE TOWER\\n\\nAnd we would know that we were\\n\\nThe storytellers of our dreams\\n\\nAnd we would never give up\\n\\nThe flag that was,\\n\\nAnd we would sit down\\n\\nTo eat lunch\\n\\nAnd the sun would be\\n\\nDull and we would be,\\n\\nAnd we would meet again\\n\\nIn a new building\\n\\nWith a new name\\n\\nAnd we would be\\n\\nmony and we would\\n\\nBeside each other\\n\\n', generation_info={'finish_reason': 'length', 'logprobs': None}), Generation(text='\\n\\nI can\\'t pick a favorite poem\\n\\nBut here are a few that come to mind\\n\\n\"I have no use for what I do not want\"\\n\\n\"The best way to do things is instead of saying no\\n\\nI just go through the motions and make the choice\"\\n\\n\"I have no use for what I do not want\"\\n\\n\"The best way to do things is instead of saying no\\n\\nI just go through the motions and make the choice\"', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWhen I was younger\\nI thought that love\\nI was something like a fairytale\\nI would find my prince and they would be my prince\\nAnd I would be able to sit up\\nAnd I was able to play without apples\\nWhen I was younger\\nI thought that love\\nI was something like a fairytale\\nI would find my prince and they would be my prince\\nAnd I would be able to sit up\\nAnd I was able to play without apples\\n\\nNow I know that love\\nIs something that should be shared\\nAnd I should be able to be myself\\nAnd be with who I want\\nNow I know that love\\nIs something that should be shared\\nAnd I should be able to be myself\\nAnd be with who I want', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text=\"\\n\\nThe world is a beautiful place\\n\\nAnd I feel so happy here\\n\\nI don't need to say\\n\\nWhat I am feeling\\n\\nIt's just there for me\\n\\nAnd I love this place\\n\\nThis beautiful way\\n\\nAnd I don't need to say\\n\\nHow much I love this place\\n\\nThis is all that matters\\n\\nAnd I'll never go to another place\\n\\nBut I'm happy here\\n\\nAnd I love this place\\n\\nThis is all that matters\", generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=\"\\n\\nWhen I was younger\\nI thought that love\\nI was something like a fairytale\\nI would find my prince\\nAnd we would be together\\nForever\\nI was naïve\\nAnd I was wrong\\nLove is not a fairytale\\nIt's something else entirely\\nSomething that should be cherished\\nAnd loved\\nAnd never taken for granted\\nLove is something that you have to work for\\nIt doesn't come easy\\nYou have to sacrifice\\nYour time, your effort\\nAnd sometimes you have to give up \\nYou have to do what's best for yourself\\nAnd sometimes that means giving love up\", generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nI can\\'t pick a love poem\\n\\nI\\'ll just go with \"gone\"\\n\\n\\n\\nI told him/she loved me\\n\\nAnd I felt so alive\\n\\nThat I couldn\\'t help but hold on\\n\\nAnd be with him/her all in one\\n\\nTime is so precious\\n\\nAnd I feel so lucky\\n\\nTo be able to be loved so much\\n\\nAnd be with him/her all in one\\n\\nTime is so precious\\n\\nAnd I feel so lucky\\n\\nTo be able to be loved so much', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWhat\\'s the best way to describe it when a girl says \"She\\'s not into you?\"\\n\\n\"She\\'s not into you, but she\\'s not into anyone else.\"', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=\"\\n\\nI\\nNo one ever really is\\n\\nThe world is so big and wide\\n\\nAnd there's so much to see\\n\\nAnd so much to do\\n\\nAnd so much to past\\n\\nIn this long long life we lead\\n\\nWe make choices that will stay\\n\\nIn our memories we keep recalling\\n\\nAll the wonderful times we've spent\\n\\nIn our hearts we know that we're wrong\\n\\nTo think that this journey's over\\n\\nWe'll find our way back to you\\n\\nIn just one more step\\n\\nIn life we make our way\\n\\nTo what's next?\\n\\nA possible future where we're always there\\n\\nA possible future where we're always present\\n\\nA possible future where we're always now\\n\\nA possible future where we're always past\\n\\nA possible future where we're always future\", generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text=\"\\n\\nA rose by the side of the road\\n\\nIs all I need to find my way\\n\\nTo the place I've been searching for\\n\\nAnd my heart is singing with joy\\n\\nWhen I look at this rose\\n\\nIt reminds me of the love I've found\\n\\nAnd I know that wherever I go\\n\\nI'll always find my rose by the side of the road.\", generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=\"\\n\\nThe world is a beautiful place,\\nThe colors are so bright and true,\\nAnd I feel so free and free,\\nWhen I'm away from here.\\n\\nThe sky is so blue,\\nAnd the sun is so warm,\\nAnd I feel so free and free,\\nWhen I'm away from here.\", generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text=\"\\n\\nWhen I was younger\\nI thought that love\\nI was something like a fairytale\\nI would find my prince\\nAnd we would be together\\nForever\\nI was naïve\\nAnd I was wrong\\nLove is not a fairytale\\nIt's something else entirely\\nSomething that should be cherished\\nAnd loved\\nAnd never taken for granted\\nLove is something that you have to work for\\nIt doesn't come easy\\nYou have to sacrifice\\nYour time, your effort\\nAnd sometimes you have to give up \\nYou have to do what's best for yourself\\nAnd sometimes that means giving love up\", generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=\"\\n\\nOnce we loved,\\n\\nBut now it's just a dream\\n\\nWe're not there,\\n\\nWe're not there\\n\\nWe're just in the air\\n\\nAnd we'll never be back\\n\\nTo that place we loved\\n\\nWe're not there,\\n\\nWe're not there\\n\\nWe're just in the air\\n\\nAnd we'll never be back\\n\\nTo that place we loved\\n\\nWe're not there,\\n\\nWe're not there\\n\\nWe're just in the air\\n\\nAnd we'll never be back\\n\\nTo that place we loved\", generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text=\"\\n\\nHow could I\\nMarry someone I don't love\\n\\nAnd not love them too\", generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=\"\\n\\nWhen I was younger\\nI thought that love\\nI was something like a fairytale\\nI would find my prince\\nAnd we would be together\\nForever\\nI was naïve\\nAnd I was wrong\\nLove is not a fairytale\\nIt's something else entirely\\nSomething that should be cherished\\nAnd loved\\nAnd never taken for granted\\nLove is something that you have to work for\\nIt doesn't come easy\\nYou have to sacrifice\\nYour time, your effort\\nAnd sometimes you have to give up \\nYou have to do what's best for yourself\\nAnd sometimes that means giving love up\", generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text=\"\\n\\nA rose by the side of the road\\n\\nIs all I need to find my way\\n\\nTo the place I've been searching for\\n\\nAnd my heart is singing with joy\\n\\nWhen I look at this rose\\n\\nIt reminds me of the love I've found\\n\\nAnd I know that wherever I go\\n\\nI'll always find my rose by the side of the road.\", generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWe are all broken\\n\\nEach piece of us\\n\\nBleeds out in the rain\\n\\nBut we try to mend\\n\\nAnd hope for a better tomorrow.', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text=\"\\n\\nWhen I was younger\\nI thought that love\\nI was something like a fairytale\\nI would find my prince\\nAnd we would be together\\nForever\\nI was naïve\\nAnd I was wrong\\nLove is not a fairytale\\nIt's something else entirely\\nSomething that should be cherished\\nAnd loved\\nAnd never taken for granted\\nLove is something that you have to work for\\nIt doesn't come easy\\nYou have to sacrifice\\nYour time, your effort\\nAnd sometimes you have to give up \\nYou have to do what's best for yourself\\nAnd sometimes that means giving love up\", generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nWhat\\'s the best way to describe it when a girl says \"She\\'s not into you?\"\\n\\n\"She\\'s not into you because she\\'s not into anyone.\"', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=\"\\n\\nThe world is a beautiful place,\\nThe colors are so bright and true,\\nAnd I feel so free and free,\\nWhen I'm away from here.\\n\\nThe sky is so blue,\\nAnd the sun is so warm,\\nAnd I feel so free and free,\\nWhen I'm away from here.\", generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text=\"\\n\\nThere's no one I want to leave\\n\\nOnly you in my heart\\n\\nAnd the happiness we'll find\\n\\nIn the darkness we'll find a way\\n\\nTo fit together and make a plan\\n\\nSo I'll never let you win\\n\\nOnly you can make me feel so low\\n\\nAnd always know that you're my everything\\n\\nOnly you can make me feel so low\\n\\nAnd always know that you're my everything\\n\\nOnly you can make me feel so low\\n\\nAnd always know that you're my everything\\n\\nOnly you can make me feel so low\\n\\nAnd always know that you're my everything\", generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'prompt_tokens': 120, 'total_tokens': 4392, 'completion_tokens': 4272}, 'model_name': 'text-ada-001'})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result = llm.generate([\"Tell me a joke\", \"Tell me a poem\"]*15)\n",
    "llm_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0010d4d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(llm_result.generations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4fab4c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Generation(text=\"\\n\\nThe world is a beautiful place,\\nThe colors are so bright and true,\\nAnd I feel so free and free,\\nWhen I'm away from here.\\n\\nThe sky is so blue,\\nAnd the sun is so warm,\\nAnd I feel so free and free,\\nWhen I'm away from here.\", generation_info={'finish_reason': 'stop', 'logprobs': None}),\n",
       " Generation(text=\"\\n\\nThere's no one I want to leave\\n\\nOnly you in my heart\\n\\nAnd the happiness we'll find\\n\\nIn the darkness we'll find a way\\n\\nTo fit together and make a plan\\n\\nSo I'll never let you win\\n\\nOnly you can make me feel so low\\n\\nAnd always know that you're my everything\\n\\nOnly you can make me feel so low\\n\\nAnd always know that you're my everything\\n\\nOnly you can make me feel so low\\n\\nAnd always know that you're my everything\\n\\nOnly you can make me feel so low\\n\\nAnd always know that you're my everything\", generation_info={'finish_reason': 'stop', 'logprobs': None})]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result.generations[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f8c31c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
